{
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "QA_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LordLean/Extracting-Green-Bonds-Use-of-Proceeds/blob/main/QA_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ICMA Database Upload"
      ],
      "metadata": {
        "id": "mu8e9txT5YaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.icmagroup.org/assets/documents/Sustainable-finance/Database/ICMA-Sustainable-Bonds-Database-120822.xlsx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8hIYyDM5ijb",
        "outputId": "e8774450-a6e3-4ccc-c0c8-b34703d4b1a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-14 11:47:18--  https://www.icmagroup.org/assets/documents/Sustainable-finance/Database/ICMA-Sustainable-Bonds-Database-120822.xlsx\n",
            "Resolving www.icmagroup.org (www.icmagroup.org)... 91.216.93.249\n",
            "Connecting to www.icmagroup.org (www.icmagroup.org)|91.216.93.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 274575 (268K) [application/vnd.openxmlformats-officedocument.spreadsheetml.sheet]\n",
            "Saving to: ‘ICMA-Sustainable-Bonds-Database-120822.xlsx’\n",
            "\n",
            "ICMA-Sustainable-Bo 100%[===================>] 268.14K   853KB/s    in 0.3s    \n",
            "\n",
            "2022-08-14 11:47:19 (853 KB/s) - ‘ICMA-Sustainable-Bonds-Database-120822.xlsx’ saved [274575/274575]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openpyxl"
      ],
      "metadata": {
        "id": "-H5NxiQp5jUo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"ICMA-Sustainable-Bonds-Database-120822.xlsx\"\n",
        "\n",
        "# select green bond sheet.\n",
        "gb_sheet = pd.ExcelFile(filename).sheet_names[0] \n",
        "\n",
        "df = pd.read_excel(filename, sheet_name=gb_sheet, header=1)"
      ],
      "metadata": {
        "id": "zB4OsFdq6Mbw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use openpyxl to load xls with hyperlink text.\n",
        "wb = openpyxl.load_workbook(filename)\n",
        "ws = wb[gb_sheet]\n",
        "\n",
        "hyperlink_list = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "  try:\n",
        "    hyperlink_list.append(ws.cell(row=(3+i), column=6).hyperlink.target)\n",
        "  except:\n",
        "    # Nan \n",
        "    hyperlink_list.append(None)\n",
        "\n",
        "# Add list to df.\n",
        "df[\"External Review Report Text\"] = hyperlink_list"
      ],
      "metadata": {
        "id": "F0kUctbB-Axb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "european = [\n",
        "    'Spain', \"The Netherlands\", \"Italy\", \"Sweden\", \"Norway\", \"France\", \"Luxembourg\",\n",
        "    \"UK\", \"Belgium\", \"Hungary\", \"Switzerland\", \"Germany\", \"Finland\", \"Iceland\", \"Poland\",\n",
        "    \"Czech Republic\", \"Denmark\", \"Ireland\", \"Greece\", \"Guernsey\", \"Austria\", \"Latvia\",\n",
        "    \"Lithuania\", \"Romania\", \"Slovenia\", \"Slovakia\",\n",
        "]\n",
        "sector = \"Corporate-Real Estate\"\n",
        "external = \"SUSTAINALYTICS\" # second-party opinion\n",
        "\n",
        "\n",
        "df = df.loc[\n",
        "    (df[\"Jurisdiction\"].isin(european)) &\n",
        "    (df[\"Issuer Category/Sector\"] == sector) &\n",
        "    (df[\"External Review Report\"] == external)\n",
        "] \n",
        "\n",
        "files = df[\"External Review Report Text\"].to_list()"
      ],
      "metadata": {
        "id": "FFkkE3I77RWz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjunsQehNYS0",
        "outputId": "a6cdd5d8-ff81-4724-fddc-e72b27dfadec"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.icmagroup.org/Emails/icma-vcards/Blackstone_External%20Review%20Report.pdf',\n",
              " 'http://www.icmagroup.org/Emails/icma-vcards/Castellum_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Cibus_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/CTP_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Deutsche%20Wohnen_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Globalworth_External%20Review%20Report.pdf',\n",
              " 'http://www.icmagroup.org/Emails/icma-vcards/ICADE_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Johnson_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Specialfastigheter-External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/tritax-eurobox_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Vesteda_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/V%C3%ADa%20C%C3%A9lere_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Vonovia_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Xior_External%20Review%20Report.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name2url = {link.strip().rsplit('/', 1)[-1] : link.strip() for link in files}\n",
        "url2name = {link.strip() : link.strip().rsplit('/', 1)[-1]for link in files}\n",
        "url2name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tD0SVtpKTmo",
        "outputId": "3ad7bdd9-a039-458d-e69c-a01108e81d3b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'http://www.icmagroup.org/Emails/icma-vcards/Castellum_External%20Review%20Report.pdf': 'Castellum_External%20Review%20Report.pdf',\n",
              " 'http://www.icmagroup.org/Emails/icma-vcards/ICADE_External%20Review%20Report.pdf': 'ICADE_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Blackstone_External%20Review%20Report.pdf': 'Blackstone_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/CTP_External%20Review%20Report.pdf': 'CTP_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Cibus_External%20Review%20Report.pdf': 'Cibus_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Deutsche%20Wohnen_External%20Review%20Report.pdf': 'Deutsche%20Wohnen_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Globalworth_External%20Review%20Report.pdf': 'Globalworth_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Johnson_External%20Review%20Report.pdf': 'Johnson_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Specialfastigheter-External%20Review%20Report.pdf': 'Specialfastigheter-External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/V%C3%ADa%20C%C3%A9lere_External%20Review%20Report.pdf': 'V%C3%ADa%20C%C3%A9lere_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Vesteda_External%20Review%20Report.pdf': 'Vesteda_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Vonovia_External%20Review%20Report.pdf': 'Vonovia_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/Xior_External%20Review%20Report.pdf': 'Xior_External%20Review%20Report.pdf',\n",
              " 'https://www.icmagroup.org/Emails/icma-vcards/tritax-eurobox_External%20Review%20Report.pdf': 'tritax-eurobox_External%20Review%20Report.pdf'}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create documents folder\n",
        "!mkdir documents"
      ],
      "metadata": {
        "id": "eeohGlppJu0z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import urllib.request\n",
        "\n",
        "for link in files:\n",
        "    link = link.strip()\n",
        "    name = link.rsplit('/', 1)[-1]\n",
        "    filename = os.path.join('./documents', name)\n",
        "    if not os.path.isfile(filename):\n",
        "        print('Downloading: ' + filename)\n",
        "        try:\n",
        "            urllib.request.urlretrieve(link, filename)\n",
        "        except Exception as inst:\n",
        "            print(inst)\n",
        "            print('  Encountered unknown error. Continuing.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf46y-5WJL_j",
        "outputId": "31ae29bb-b7f5-481c-f4f2-9cc4300d1652"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: ./documents/Blackstone_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/Castellum_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/Cibus_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/CTP_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/Deutsche%20Wohnen_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/Globalworth_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/ICADE_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/Johnson_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/Specialfastigheter-External%20Review%20Report.pdf\n",
            "Downloading: ./documents/tritax-eurobox_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/Vesteda_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/V%C3%ADa%20C%C3%A9lere_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/Vonovia_External%20Review%20Report.pdf\n",
            "Downloading: ./documents/Xior_External%20Review%20Report.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information Retieval"
      ],
      "metadata": {
        "id": "L8R_bhw90KQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer Retriever\n"
      ],
      "metadata": {
        "id": "eM8vWuo-yz5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank-bm25\n",
        "\n",
        "!pip install PyPDF2\n",
        "\n",
        "!pip install tabula-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a90GXSgeKsMc",
        "outputId": "9c152871-5185-40af-9b17-e864bfd1b8b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank-bm25) (1.21.6)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-2.10.0-py3-none-any.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 30.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.5.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 21.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from tabula-py) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tabula-py) (1.21.6)\n",
            "Collecting distro\n",
            "  Downloading distro-1.7.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->tabula-py) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.3->tabula-py) (1.15.0)\n",
            "Installing collected packages: distro, tabula-py\n",
            "Successfully installed distro-1.7.0 tabula-py-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import tabula\n",
        "from rank_bm25 import BM25Okapi\n",
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "Ad0C5vgsMaec"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TableReader:\n",
        "\n",
        "  def __init__(self, pdf):\n",
        "    self.pdf = pdf\n",
        "    self.dfs = None\n",
        "\n",
        "  def read_pages(self, pages=\"all\", multiple_tables=True, stream=True):\n",
        "    '''\n",
        "    Return tables discovered within pdf.\n",
        "    '''\n",
        "    self.dfs = tabula.read_pdf(self.pdf, pages=pages, multiple_tables=multiple_tables, stream=stream)\n",
        "    self.__clean_dfs()\n",
        "    return self.dfs\n",
        "\n",
        "  def __clean_dfs(self, thresh=2):\n",
        "    self.dfs = [df.dropna(thresh=thresh) for df in self.dfs]\n",
        "\n",
        "\n",
        "class Reader:\n",
        "\n",
        "  def __init__(self, filename):\n",
        "    self.reader = PdfReader(filename)\n",
        "    self.tb = TableReader(filename)\n",
        "    self.page_viewer = {page_num : {} for page_num in range(self.reader.numPages)}\n",
        "    self.idx2page_item = []\n",
        "  \n",
        "  def __extract_text(self,):\n",
        "    '''\n",
        "    Page-wise text extraction and tokenize for BM25.\n",
        "    '''\n",
        "    text_index_mem = 0\n",
        "    # List to store each tokenized corpus\n",
        "    tokenized_corpus_list = []\n",
        "    for i in range(self.reader.numPages):\n",
        "      raw_text = self.reader.getPage(i).extractText()\n",
        "      self.page_viewer[i][\"raw_text\"] = raw_text\n",
        "      # Split text\n",
        "      corpus = raw_text.split(\"\\n \\n\")\n",
        "      # Store results.\n",
        "      self.page_viewer[i][\"corpus\"] = corpus\n",
        "      for item in corpus:\n",
        "        self.idx2page_item.append((i, item)) # page,textItem\n",
        "      # Tokenize\n",
        "      tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "      tokenized_corpus_list.append(tokenized_corpus)\n",
        "    # BM25 computations only after the complete tokenized corpus is collated. \n",
        "    # Merge tokenized corpus'.\n",
        "    tokenized_corpus_complete = [item for sublist in tokenized_corpus_list for item in sublist]\n",
        "    # BM25\n",
        "    self.bm25 = BM25Okapi(tokenized_corpus_complete)\n",
        "\n",
        "  def __extract_tables(self):\n",
        "    '''\n",
        "    Page-wise table extractor.\n",
        "    '''\n",
        "    for i in range(self.reader.numPages):\n",
        "      # page=0 will throw error using tabula.\n",
        "      page = str(i+1)\n",
        "      self.page_viewer[i][\"tables\"] = self.tb.read_pages(pages=page)\n",
        "\n",
        "  def extract_pdf(self):\n",
        "    # Extract data\n",
        "    self.__extract_text()\n",
        "    self.__extract_tables()\n",
        "\n",
        "  def print_page(self, page_num):\n",
        "    '''\n",
        "    Print separated sections of text given a page.\n",
        "    '''\n",
        "    corpus = self.page_viewer[page_num][\"corpus\"]\n",
        "    for item in (corpus):\n",
        "      print(\"\\n{}\\n\".format(\"-\"*60))\n",
        "      print(item)\n",
        "    print(\"\\n{}\\n\".format(\"-\"*60))\n",
        "    for df in self.page_viewer[page_num][\"tables\"]:\n",
        "      print(df.style)\n",
        "      display(df)\n",
        "\n",
        "  def __score(self, queries, weights):\n",
        "    '''\n",
        "    Compute the average BM25 score of each given query on each page of text.\n",
        "    '''\n",
        "    self.ranked_scores = []\n",
        "    for query in queries:\n",
        "      # tokenize query by whitespace.\n",
        "      tokenized_query = query.split()\n",
        "      # Compute score.\n",
        "      doc_scores = self.bm25.get_scores(tokenized_query)\n",
        "      self.ranked_scores.append(doc_scores)\n",
        "    # Compute average (weighted) score against all queries.\n",
        "    if not len(weights):\n",
        "      # Equal weighting.\n",
        "      self.average_score = np.average(self.ranked_scores, axis=0)\n",
        "    elif len(queries) != len(weights):\n",
        "        # Unequal number of elements.\n",
        "        raise ValueError(\"Number of query and weight elements passed must be equal.\")\n",
        "    else:\n",
        "      # Weighted average.\n",
        "      self.average_score = np.average(self.ranked_scores, weights=weights, axis=0)\n",
        " \n",
        "  def get_ranked_texts(self, queries, weights=[], n=5):\n",
        "    '''\n",
        "    Return n pages which scored highest using BM25.\n",
        "    '''\n",
        "    # Run score method to calculate BM25.\n",
        "    self.__score(queries, weights)\n",
        "    idx = sorted(range(len(self.average_score)), key=lambda i: self.average_score[i], reverse=True)[:n]\n",
        "\n",
        "    final_results = []\n",
        "    for i in range(n):\n",
        "      page_num, text = self.idx2page_item[idx[i]]\n",
        "      tables = self.page_viewer[page_num][\"tables\"]\n",
        "      final_results.append({\"page_num\":page_num, \"text\":text, \"tables\":tables})\n",
        "\n",
        "    return final_results\n",
        "    "
      ],
      "metadata": {
        "id": "6vFa_DjDU15_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer Re-ranker (Neural: BERT / T5)"
      ],
      "metadata": {
        "id": "2BnnWZHubMLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygaggle\n",
        "\n",
        "!pip install transformers==4.6.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMzoVRC4bQA4",
        "outputId": "6c03842f-6d7d-479c-9068-8eccdd9690d9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygaggle\n",
            "  Downloading pygaggle-0.0.3.1.tar.gz (33 kB)\n",
            "Collecting coloredlogs==14.0\n",
            "  Downloading coloredlogs-14.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from pygaggle) (1.21.6)\n",
            "Collecting pydantic==1.5\n",
            "  Downloading pydantic-1.5-cp37-cp37m-manylinux2014_x86_64.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 46.6 MB/s \n",
            "\u001b[?25hCollecting pyserini==0.10.1.0\n",
            "  Downloading pyserini-0.10.1.0-py3-none-any.whl (63.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.3 MB 13 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from pygaggle) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from pygaggle) (1.7.3)\n",
            "Collecting spacy==2.2.4\n",
            "  Downloading spacy-2.2.4-cp37-cp37m-manylinux1_x86_64.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pygaggle) (2.8.0)\n",
            "Requirement already satisfied: tensorflow>=2.2.0rc1 in /usr/local/lib/python3.7/dist-packages (from pygaggle) (2.8.2+zzzcolab20220719082949)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 51.7 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.45.0\n",
            "  Downloading tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting transformers==4.0.0\n",
            "  Downloading transformers-4.0.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 33.9 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.94\n",
            "  Downloading sentencepiece-0.1.94-cp37-cp37m-manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.5 MB/s \n",
            "\u001b[?25hCollecting humanfriendly>=7.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from pyserini==0.10.1.0->pygaggle) (0.29.32)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pyserini==0.10.1.0->pygaggle) (1.3.5)\n",
            "Collecting pyjnius\n",
            "  Downloading pyjnius-1.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 19.6 MB/s \n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 34.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->pygaggle) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->pygaggle) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->pygaggle) (1.0.7)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 37.7 MB/s \n",
            "\u001b[?25hCollecting thinc==7.4.0\n",
            "  Downloading thinc-7.4.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 43.7 MB/s \n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->pygaggle) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->pygaggle) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->pygaggle) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.0->pygaggle) (2022.6.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.0->pygaggle) (3.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.0->pygaggle) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4->pygaggle) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4->pygaggle) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4->pygaggle) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->pygaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->pygaggle) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->pygaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->pygaggle) (2022.6.15)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->pygaggle) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->pygaggle) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.1.0->pygaggle) (1.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.1.0->pygaggle) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.1.0->pygaggle) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.1.0->pygaggle) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.1.0->pygaggle) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.1.0->pygaggle) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.1.0->pygaggle) (1.47.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.1.0->pygaggle) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.1.0->pygaggle) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.1.0->pygaggle) (1.0.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.1.0->pygaggle) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.1.0->pygaggle) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.1.0->pygaggle) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.1.0->pygaggle) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.1.0->pygaggle) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.1.0->pygaggle) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.1.0->pygaggle) (3.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (3.3.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (3.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (14.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (0.5.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0rc1->pygaggle) (0.26.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2.0rc1->pygaggle) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.0.0->pygaggle) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyserini==0.10.1.0->pygaggle) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyserini==0.10.1.0->pygaggle) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.0->pygaggle) (7.1.2)\n",
            "Building wheels for collected packages: pygaggle, sacremoses\n",
            "  Building wheel for pygaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygaggle: filename=pygaggle-0.0.3.1-py3-none-any.whl size=51498 sha256=8fd96f61ef293e0737af9131419713ced7880eee14c588ac065e56cdaa12556e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/b7/35/6b19949b191b7b5005739e020b91b3a625c3a588a0bd49d70c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=560b4696606b061aa0ee6d1d52807dd2b9926e51ea31e5e35876e694dfed753c\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built pygaggle sacremoses\n",
            "Installing collected packages: tqdm, srsly, plac, catalogue, blis, tokenizers, thinc, sacremoses, pyjnius, humanfriendly, faiss-cpu, transformers, spacy, sentencepiece, pyserini, pydantic, coloredlogs, pygaggle\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.9.1\n",
            "    Uninstalling pydantic-1.9.1:\n",
            "      Successfully uninstalled pydantic-1.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.45.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.4 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 coloredlogs-14.0 faiss-cpu-1.7.2 humanfriendly-10.0 plac-1.1.3 pydantic-1.5 pygaggle-0.0.3.1 pyjnius-1.4.2 pyserini-0.10.1.0 sacremoses-0.0.53 sentencepiece-0.1.94 spacy-2.2.4 srsly-1.0.5 thinc-7.4.0 tokenizers-0.9.4 tqdm-4.45.0 transformers-4.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.6.1\n",
            "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 22.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (0.0.53)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (4.12.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (4.45.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.1) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.1) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.1) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (1.1.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.9.4\n",
            "    Uninstalling tokenizers-0.9.4:\n",
            "      Successfully uninstalled tokenizers-0.9.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.0.0\n",
            "    Uninstalling transformers-4.0.0:\n",
            "      Successfully uninstalled transformers-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pygaggle 0.0.3.1 requires tokenizers==0.9.4, but you have tokenizers 0.10.3 which is incompatible.\n",
            "pygaggle 0.0.3.1 requires transformers==4.0.0, but you have transformers 4.6.1 which is incompatible.\u001b[0m\n",
            "Successfully installed huggingface-hub-0.0.8 tokenizers-0.10.3 transformers-4.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pygaggle.rerank.base import Query, Text\n",
        "from pygaggle.rerank.transformer import MonoT5, MonoBERT\n",
        "\n",
        "class Reranker:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.mono5t = MonoT5()\n",
        "    self.monobert = MonoBERT()\n",
        "\n",
        "  def rerank(self, query, texts, method=\"T5\"):\n",
        "    query = Query(query)\n",
        "    texts = [Text(text, {\"docid\" : i}, 0) for i, text in enumerate(texts)]\n",
        "\n",
        "    if method == \"T5\":\n",
        "      reranker = self.mono5t\n",
        "    if method == \"BERT\":\n",
        "      reranker = self.monobert\n",
        "\n",
        "    reranked = reranker.rerank(query, texts)\n",
        "    reranked.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "    return reranked"
      ],
      "metadata": {
        "id": "HM4Qf8PmbTy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80a8ced-fa8c-4def-8c00-d82cabb16373"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-14 11:52:29 [INFO] loader: Loading faiss with AVX2 support.\n",
            "2022-08-14 11:52:29 [INFO] loader: Could not load library with AVX2 support due to:\n",
            "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
            "2022-08-14 11:52:29 [INFO] loader: Loading faiss.\n",
            "2022-08-14 11:52:29 [INFO] loader: Successfully loaded faiss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QA Model"
      ],
      "metadata": {
        "id": "stSjl2v60UOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download zipped model\n",
        "!gdown 1NBc9MfT4FuchadevFJvcfyBfpIPMILb0\n",
        "\n",
        "# Unzip\n",
        "!unzip finbert-pretrain-finetuned-squad.zip\n",
        "\n",
        "# Delete\n",
        "!rm finbert-pretrain-finetuned-squad.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjCOuqJr0V7c",
        "outputId": "fc981bed-6c6f-43dd-90e8-77cc1875f29b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NBc9MfT4FuchadevFJvcfyBfpIPMILb0\n",
            "To: /content/finbert-pretrain-finetuned-squad.zip\n",
            "100% 406M/406M [00:10<00:00, 40.3MB/s]\n",
            "Archive:  finbert-pretrain-finetuned-squad.zip\n",
            "  inflating: finbert-pretrain-finetuned-squad/config.json  \n",
            "  inflating: finbert-pretrain-finetuned-squad/pytorch_model.bin  \n",
            "  inflating: finbert-pretrain-finetuned-squad/special_tokens_map.json  \n",
            "  inflating: finbert-pretrain-finetuned-squad/tokenizer.json  \n",
            "  inflating: finbert-pretrain-finetuned-squad/tokenizer_config.json  \n",
            "  inflating: finbert-pretrain-finetuned-squad/training_args.bin  \n",
            "  inflating: finbert-pretrain-finetuned-squad/vocab.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers \n",
        "\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "GaRQa9-U0qwe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"finbert-pretrain-finetuned-squad\"\n",
        "\n",
        "question_answering = pipeline(\"question-answering\", model=model_dir, tokenizer=model_dir)"
      ],
      "metadata": {
        "id": "NcRV_F-H08df"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"\n",
        "Machine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\n",
        "\"\"\"\n",
        "question = \"what are the machine learning models based on?\"\n",
        "\n",
        "question, context = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\""
      ],
      "metadata": {
        "id": "ocIsSumr1Haf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"\n",
        "Vía Célere intends to report on allocation of proceeds on\n",
        "its website, on an annual basis, until full allocation. The allocation\n",
        "reporting will include the total amount allocated to projects, the share\n",
        "of financing vs. refinancing, and unallocated proceeds. In addition,\n",
        "Vía Célere is committed to reporting on relevant impact metrics, such\n",
        "as energy consumption reduction (in kWh) or emission reduction (in\n",
        "tons of CO2e). Sustainalytics views Vía Célere’s allocation and impact\n",
        "reporting as aligned with market practice.\n",
        "\"\"\"\n",
        "context2 = \"\"\"\n",
        "Blackstone Property Partners Europe Holdings intends\n",
        "to report on allocation of proceeds on its website on an annual basis\n",
        "until full allocation or while financing instruments remain\n",
        "outstanding. In addition, to the extent practicable, the Company\n",
        "intends to report on relevant impact metrics such as green building\n",
        "certification level, renewable energy installed capacity and annual\n",
        "energy savings. Sustainalytics views the allocation and impact\n",
        "reporting as aligned with market practice.\n",
        "\"\"\"\n",
        "\n",
        "question = \"how often is allocation of proceeds reported?\""
      ],
      "metadata": {
        "id": "ZW7GHrHWO5Kq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = question_answering(question=question, context=context, device=0)\n",
        "print(\"Answer:\", result['answer'])\n",
        "print(\"Score:\", result['score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4ljNTRZ1Kmo",
        "outputId": "5cefc5c3-b1aa-4184-d2e5-2eda5fc52afc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: annual basis, until full allocation\n",
            "Score: 0.20437736809253693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "oDZ9utOmMp7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"documents/{}\".format(url2name[files[0]])\n",
        "reader = Reader(filename)\n",
        "\n",
        "reader.extract_pdf()\n",
        "queries = [\n",
        "    \"use of proceeds\",\n",
        "    \"allocation of proceeds\",\n",
        "    ]"
      ],
      "metadata": {
        "id": "t2adBki8MgxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a1993e-4069-4523-d979-5e6ed723114a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-14 11:56:46 [WARNING] _utils:  impossible to decode XFormObject /Meta29\n",
            "2022-08-14 11:56:46 [WARNING] _utils:  impossible to decode XFormObject /Meta30\n",
            "2022-08-14 11:56:46 [WARNING] _utils:  impossible to decode XFormObject /Meta31\n",
            "2022-08-14 11:56:46 [WARNING] _utils:  impossible to decode XFormObject /Meta32\n",
            "2022-08-14 11:57:00 [WARNING] io: Got stderr: Aug 14, 2022 11:56:57 AM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider loadDiskCache\n",
            "WARNING: New fonts found, font cache will be re-built\n",
            "Aug 14, 2022 11:56:57 AM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider <init>\n",
            "WARNING: Building on-disk font cache, this may take a while\n",
            "Aug 14, 2022 11:56:57 AM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider <init>\n",
            "WARNING: Finished building on-disk font cache, found 17 fonts\n",
            "\n",
            "2022-08-14 11:57:37 [WARNING] io: Got stderr: Aug 14, 2022 11:57:35 AM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
            "WARNING: Format 14 cmap table is not supported and will be ignored\n",
            "Aug 14, 2022 11:57:36 AM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
            "WARNING: Format 14 cmap table is not supported and will be ignored\n",
            "\n",
            "2022-08-14 11:57:40 [WARNING] io: Got stderr: Aug 14, 2022 11:57:38 AM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
            "WARNING: Format 14 cmap table is not supported and will be ignored\n",
            "Aug 14, 2022 11:57:40 AM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
            "WARNING: Format 14 cmap table is not supported and will be ignored\n",
            "\n",
            "2022-08-14 11:57:43 [WARNING] io: Got stderr: Aug 14, 2022 11:57:41 AM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
            "WARNING: Format 14 cmap table is not supported and will be ignored\n",
            "Aug 14, 2022 11:57:42 AM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
            "WARNING: Format 14 cmap table is not supported and will be ignored\n",
            "\n",
            "2022-08-14 11:57:46 [WARNING] io: Got stderr: Aug 14, 2022 11:57:44 AM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
            "WARNING: Format 14 cmap table is not supported and will be ignored\n",
            "Aug 14, 2022 11:57:45 AM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
            "WARNING: Format 14 cmap table is not supported and will be ignored\n",
            "\n",
            "2022-08-14 11:57:48 [WARNING] io: Got stderr: Aug 14, 2022 11:57:47 AM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
            "WARNING: Format 14 cmap table is not supported and will be ignored\n",
            "Aug 14, 2022 11:57:48 AM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
            "WARNING: Format 14 cmap table is not supported and will be ignored\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reranker = Reranker()\n",
        "\n",
        "query = \"what did the use of proceeds finance\"\n",
        "\n",
        "texts = [item[\"text\"] for item in top_items]\n",
        "\n",
        "reranked = reranker.rerank(query, texts, method=\"T5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJwTmWmxjqBi",
        "outputId": "b96031c9-6e37-4635-913c-e579801ca76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:173: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n"
          ]
        }
      ]
    }
  ]
}